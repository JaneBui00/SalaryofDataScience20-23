{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64bb8ff",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b73dd",
   "metadata": {},
   "source": [
    "## 1. Define problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f689d",
   "metadata": {},
   "source": [
    "As you know, we are living in an era of digital data abundance and technical involving extracting knowledge or useful insights from those data is necessary for adoption intelligent solutions in various application areas. Thus, the data science industry is playing an increasingly important role, and its salaries also are becoming more and more attractive. \n",
    "In this analysis, I will: \n",
    "- Explore the factors directly impacting data science industry salaries over the past 4 years.\n",
    "- Train a model with these factors to predict the salary in near future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088818c",
   "metadata": {},
   "source": [
    "## 2. Link for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ecff8",
   "metadata": {},
   "source": [
    " https://www.kaggle.com/datasets/iamsouravbanerjee/data-science-salaries-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73144d3f",
   "metadata": {},
   "source": [
    "## 3. Features of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31768ff",
   "metadata": {},
   "source": [
    "1. job_title: Data related professions\n",
    "2. employment_type: Type of employment\n",
    "    - Full-Time\n",
    "    - Part-Time\n",
    "    - Contract\n",
    "    - Freelance\n",
    "3. experience_level: Experience Level\n",
    "    - Entry\n",
    "    - Medium\n",
    "    - Senior\n",
    "    - Executive\n",
    "4. company_location: Country of the employer's head office or branch office\n",
    "5. company_size: Average number of employees during the year\n",
    "    - Small: less than 50 employees\n",
    "    - Medium: from 50 to 250 employees\n",
    "    - Large: more than 250 employees\n",
    "6. year: The year in which the salary was paid\n",
    "    - 2020\n",
    "    - 2021\n",
    "    - 2022\n",
    "    - 2023\n",
    "7. salary_in_usd: Salaries in US dollars (currency rate divided by the average US dollar rate for the corresponding year via fxdata.foorilla.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb9172",
   "metadata": {},
   "source": [
    "## 4. Import neccessary libraries for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5eb860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for basics purposes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statistics\n",
    "\n",
    "# for machine learning\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# for evaluation model\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, \\\n",
    "                            median_absolute_error, max_error,confusion_matrix, ConfusionMatrixDisplay\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6ae82",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e7294e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import data and exclude redundant columns\n",
    "sal = pd.read_csv('Latest_Data_Science_Salaries.csv')\n",
    "sal.columns = sal.columns.str.lower()\n",
    "sal.columns = sal.columns.str.replace(\" \",\"_\")\n",
    "sal.drop(['expertise_level', 'salary','salary_currency', 'employee_residence'], axis = 1, inplace = True)\n",
    "sal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf0bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the dtype of each column\n",
    "sal.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430c1a03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We see that some title written in full name, but some in abbreviated name\n",
    "a = sal['job_title'].unique()\n",
    "np.sort(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5674b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.job_title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176a8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we need to change some full-name titles to abbreviated name\n",
    "\n",
    "bi_da_an = ['BI Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst'] #change to 'BI Data Analyst'\n",
    "bi_da_en = ['Business Intelligence Engineer'] #change to 'BI Data Engineer'\n",
    "da_en = ['Data Engineer 2'] # change to 'Data Engineer'\n",
    "da_mo = ['Data Modeller'] # change to 'Data Modeler'\n",
    "fi_da_an = ['Finance Data Analyst'] # change to 'Financial Data Analyst'\n",
    "he_of_da = ['Head of Data Science'] # change to 'Head of Data'\n",
    "ml_en = ['Machine Learning Engineer'] # change to 'ML Engineer'\n",
    "\n",
    "change = {'BI Data Analyst':bi_da_an,\n",
    "          'BI Data Engineer':bi_da_en,\n",
    "          'Data Engineer':da_en,\n",
    "          'Data Modeler':da_mo,\n",
    "          'Financial Data Analyst':fi_da_an,\n",
    "          'Head of Data':he_of_da,\n",
    "          'ML Engineer':ml_en}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8417e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create function to change title easily\n",
    "def change_title(changefrom, changeto):\n",
    "    indexes = []\n",
    "    for word in changefrom:\n",
    "        ind = sal[sal['job_title'].str.contains(word)].index\n",
    "        indexes.extend(ind)\n",
    "    \n",
    "    sal.loc[indexes,'job_title'] = changeto\n",
    "    return sal\n",
    "\n",
    "for name, values in change.items():\n",
    "    change_title(changefrom = values, changeto = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad8b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we just only have 98 job titles\n",
    "sal.job_title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1951ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cop = sal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c93e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the distributon of salary \n",
    "\n",
    "min_sal = min(cop['salary_in_usd'])\n",
    "max_sal = max(cop['salary_in_usd'])\n",
    "print('Minimum salary: ', min_sal)\n",
    "print('Maximum salary: ', max_sal)\n",
    "\n",
    "a = [round(q) for q in statistics.quantiles(cop['salary_in_usd'], n=4)]\n",
    "quan = pd.DataFrame({'Percent': ['25%', '50%', '75%'], \n",
    "                     'Salary': a})\n",
    "print('\\nInterquantile of the salary')\n",
    "quan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a817dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the salary distributon \n",
    "\n",
    "sns.color_palette(\"muted\")\n",
    "\n",
    "#Create histogram\n",
    "fig, ax1 = plt.subplots(figsize = (10, 5))\n",
    "ax1.hist(cop['salary_in_usd'], bins = 30, color = 'forestgreen')\n",
    "\n",
    "#Create the density\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Salary Density')\n",
    "sns.kdeplot(cop['salary_in_usd'], color = 'darkgreen')\n",
    "\n",
    "# Line 25%\n",
    "ax1.axvline(x = 90000, linestyle = '-', color = 'black', ls ='--')\n",
    "ax1.annotate('25% vacancies \\n have a salary \\n less than $90,000',\n",
    "             xy = (45000, 260),\n",
    "             color = 'black', ha=\"center\", va=\"center\")\n",
    "\n",
    "# Line 75%\n",
    "ax1.axvline(x = 185000, linestyle = '-', color = 'black', ls ='--')\n",
    "ax1.annotate('25% vacancies \\n have a salary \\n more than $185,000',\n",
    "             xy = (235000, 260),\n",
    "             color = 'black', ha=\"center\", va=\"center\")\n",
    "\n",
    "ax1.set_ylabel('Number of vacancies')\n",
    "ax1.set_xlabel('Salary in data science industry')\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "plt.title('Salary distribution', fontsize = 13, fontweight = 'bold')\n",
    "plt.xlim(0,480000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21010230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the titles of the most 15 job in-demand\n",
    "top15 = cop['job_title'].value_counts(sort=True)[:15]\n",
    "\n",
    "#create df of top 15\n",
    "df_15 = cop[cop['job_title'].isin(top15.index)]\n",
    "sal_job15 = df_15.groupby(by = 'job_title').agg({'salary_in_usd': 'mean','job_title': 'count'} ) \n",
    "sal_job15['salary_in_usd'] = [round(i) for i in sal_job15['salary_in_usd']]\n",
    "sal_job15.columns = ['Salary in year', 'Number of vacancies']\n",
    "sal_job15.sort_values(by = 'Number of vacancies', ascending = False, inplace = True)\n",
    "\n",
    "# sal_job15.drop('year', axis = 1, inplace = True)\n",
    "\n",
    "sal_job15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92254da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(figsize = (15,7),\n",
    "                              sharex = True, \n",
    "                              nrows = 2, \n",
    "                              ncols = 1)\n",
    "\n",
    "## The first plot\n",
    "ax1.bar(height = sal_job15['Number of vacancies'] , \n",
    "        x = sal_job15.index, \n",
    "        color = 'forestgreen')\n",
    "\n",
    "ax1.set_ylabel('Number of jobs')\n",
    "ax1.set_title('Number of jobs and wages by “Position”', fontsize = 13, fontweight = 'bold')\n",
    "\n",
    "## The second plot\n",
    "sns.boxplot(y = df_15['salary_in_usd'],\n",
    "            x = df_15['job_title'], \n",
    "            order = sal_job15.index,  \n",
    "            ax = ax2,\n",
    "            palette = sns.color_palette(\"Paired\"))\n",
    "\n",
    "ax2.set_ylabel('Salary in a year ($)')\n",
    "ax2.set_xlabel(None)\n",
    "\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "plt.xticks(rotation = 75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2edd72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change type of dataframe\n",
    "cat_cols = ['job_title', 'employment_type', 'experience_level', 'company_location', 'company_size']\n",
    "for col in cat_cols:\n",
    "    cop[col] = cop[col].astype('category')\n",
    "\n",
    "cop['year'] = cop['year'].astype('string') # we change type of column year to string so that xticks won't be decimal number  \n",
    "cop.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d44ad4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of jobs and salary of top 10 by Company Location\n",
    "\n",
    "## Create data frame\n",
    "asd = cop.groupby('company_location').agg({'salary_in_usd':'mean', 'company_location':'count'})\n",
    "asd.columns = ['salary', 'location']\n",
    "asd.sort_values(by= 'location', ascending = False, inplace = True)\n",
    "asd = asd.iloc[:10]\n",
    "\n",
    "## Plot\n",
    "fig, ax1 = plt.subplots(figsize = (15, 7))\n",
    "plt.title('Number of jobs and wages by “Company location”', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "\n",
    "ax1.bar(x = asd.index, height = asd['location'], color='forestgreen')\n",
    "ax1.set_ylabel('Number of jobs')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(asd.index, asd['salary'], color = 'black', marker = 'o')\n",
    "ax2.set_ylabel('Salary in a year ($)', color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "ax2.legend('$')\n",
    "\n",
    "ax1.grid(False, axis = 'both')\n",
    "ax2.grid(False, axis = 'both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b6d2973",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot for 4 variables by Number of vacancies\n",
    "\n",
    "## Create data\n",
    "exp = cop['experience_level'].value_counts()\n",
    "emp = cop['employment_type'].value_counts()\n",
    "comsize = cop['company_size'].value_counts()\n",
    "yer = cop['year'].value_counts()\n",
    "\n",
    "\n",
    "## Plot \n",
    "fig, ((ax1,ax2),(ax3, ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (15,7))\n",
    "fig.suptitle('Number of vacancies by four factors', \n",
    "             fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "\n",
    "ax1.bar(height = emp, x = emp.index, color = 'forestgreen')\n",
    "ax1.set_ylabel('Number of vacancies')\n",
    "ax1.set_title('by “Type of employment”', fontsize = 14, fontweight = 'bold')\n",
    "ax1.grid(False)\n",
    "for i in range(len(emp.index)):\n",
    "    ax1.text(i-0.1,emp[i]+25,emp[i], fontsize = 9)\n",
    "\n",
    "\n",
    "ax2.bar(height = exp, x = exp.index, color = 'forestgreen')\n",
    "ax2.set_ylabel('Number of vacancies')\n",
    "ax2.set_title('by \"Experience Level\"', fontsize = 14, fontweight = 'bold')\n",
    "ax2.grid(False)\n",
    "for i in range(len(exp.index)):\n",
    "    ax2.text(i-0.1,exp[i]+15,exp[i], fontsize = 9)\n",
    "    \n",
    "    \n",
    "ax3.bar(height = comsize, x = comsize.index, color = 'forestgreen')\n",
    "ax3.set_ylabel('Number of vacancies')\n",
    "ax3.set_title('by \"Company Size\"', fontsize = 14, fontweight = 'bold')\n",
    "ax3.grid(False)\n",
    "for i in range(len(comsize.index)):\n",
    "    ax3.text(i-0.07,comsize[i]+15,comsize[i], fontsize = 9)\n",
    "\n",
    "    \n",
    "ax4.bar(height = yer, x = yer.index, color = 'forestgreen')\n",
    "ax4.set_ylabel('Number of vacancies')\n",
    "ax4.set_title('by \"Year\"', fontsize = 14, fontweight = 'bold')\n",
    "ax4.grid(False)\n",
    "for i in range(len(yer.index)):\n",
    "    ax4.text(i-0.1,yer[i]+15,yer[i], fontsize = 9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14bbe6d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for 4 variables by Salary\n",
    "fig, ((ax1,ax2),(ax3, ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (15,7))\n",
    "fig.suptitle('Salary by four factors', fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "sns.boxplot(data = cop, y = 'salary_in_usd', x = 'employment_type', palette = sns.color_palette(\"Paired\"), \n",
    "            order = ['Full-Time', 'Contract', 'Part-Time',  'Freelance'], ax = ax1)\n",
    "ax1.set_ylabel('Salary in a year ($)')\n",
    "ax1.set_xlabel(None)\n",
    "ax1.set_yticks(np.arange(0, 500000, step=50000))\n",
    "ax1.set_title('by \"Type of Employment\"', fontsize = 14, fontweight = 'bold')\n",
    "ax1.grid(False)\n",
    "\n",
    "sns.boxplot(data = cop, y = 'salary_in_usd', x = 'experience_level',palette = sns.color_palette(\"Paired\"),\n",
    "            order = [ 'Entry', 'Mid','Senior', 'Executive'], ax = ax2)\n",
    "ax2.set_ylabel('Salary in a year ($)')\n",
    "ax2.set_xlabel(None)\n",
    "ax2.set_yticks(np.arange(0, 500000, step=50000)) \n",
    "ax2.set_title('by \"Level of Experience\"', fontsize = 14, fontweight = 'bold')\n",
    "ax2.grid(False)\n",
    "\n",
    "\n",
    "sns.boxplot(data = cop, y = 'salary_in_usd', x = 'company_size', \n",
    "            palette = sns.color_palette(\"Paired\"), order = ['Small', 'Medium', 'Large'], ax = ax3)\n",
    "ax3.set_xlabel(None)\n",
    "ax3.set_ylabel('Salary in a year ($)')\n",
    "ax3.set_title('by \"Size of Company\"', fontsize = 14, fontweight = 'bold')\n",
    "ax3.grid(False)\n",
    "\n",
    "\n",
    "sns.boxplot(data = cop, y = 'salary_in_usd', x = 'year',\n",
    "            palette = sns.color_palette(\"Paired\"), order = ['2020', '2021', '2022', '2023'], ax = ax4)\n",
    "ax4.set_xlabel(None)\n",
    "ax4.set_ylabel('Salary in a year ($)', fontsize = 12)\n",
    "ax4.set_title('by \"Year\"', fontsize = 14, fontweight = 'bold')\n",
    "ax4.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3b008",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3354e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create function for test Kruskal-Wallis\n",
    "def kruskal(variable):\n",
    "    unique = cop[variable].unique()\n",
    "    num_unique = cop[variable].nunique()\n",
    "    if num_unique == 3:\n",
    "        var0 = cop['salary_in_usd'][cop[variable] == unique[0]].to_list()\n",
    "        var1 = cop['salary_in_usd'][cop[variable] == unique[1]].to_list()\n",
    "        var2 = cop['salary_in_usd'][cop[variable] == unique[2]].to_list()\n",
    "        result = stats.kruskal(var0, var1, var2)\n",
    "\n",
    "    elif num_unique == 4:\n",
    "        var0 = cop['salary_in_usd'][cop[variable] == unique[0]].to_list()\n",
    "        var1 = cop['salary_in_usd'][cop[variable] == unique[1]].to_list()\n",
    "        var2 = cop['salary_in_usd'][cop[variable] == unique[2]].to_list()\n",
    "        var3 = cop['salary_in_usd'][cop[variable] == unique[3]].to_list()\n",
    "        result = stats.kruskal(var0, var1, var2,var3)\n",
    "    \n",
    "    # Print the result\n",
    "    print('\\nThe variable', str.upper(variable), 'has result:')\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3d267c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kruskal('year'), kruskal('company_size'), kruskal('employment_type'), kruskal('experience_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e42eb",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a94518de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "copp = cop.copy()\n",
    "copp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5f6b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "copp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c4f7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all category columns to numeric\n",
    "to_onehot = ['employment_type', 'experience_level', 'company_size', 'year','job_title', 'company_location']\n",
    "\n",
    "onehot = OneHotEncoder(sparse_output = False)\n",
    "trans = ColumnTransformer([('onehot', onehot, to_onehot)], \n",
    "                          remainder = 'passthrough')\n",
    "copp_trans = trans.fit_transform(copp)\n",
    "copp_df = pd.DataFrame(copp_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ef8b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "copp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59cf5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = copp_df.drop(184, axis = 1)\n",
    "y = copp_df[184]\n",
    "\n",
    "np.random.seed(100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f51b66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Ridge': Ridge(max_iter = 3000, alpha = 4.0, solver='sag'),\n",
    "         'Lasso': Lasso(max_iter = 3000, alpha = 4.0),\n",
    "         'ElasticNet': ElasticNet(),\n",
    "         'RandomForestRegressor': RandomForestRegressor()}\n",
    "for name, model in models.items():\n",
    "    model_use = model\n",
    "    model_use.fit(X_train, y_train)\n",
    "    score = model_use.score(X_test, y_test)\n",
    "    print(f'{name} has score: {score :.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f245bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "296207b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Value of r2_score: {r2_score(y_test, y_preds) :.3f} \\n \\\n",
    "Value of mean_absolute_error: {mean_absolute_error(y_test, y_preds) :.3f} \\n \\\n",
    "Value of mean_squared_error : {mean_squared_error(y_test, y_preds) :.3f} \\n \\\n",
    "Value of median_absolute_error: {median_absolute_error(y_test, y_preds) :.3f} \\n \\\n",
    "Value of cross_val_score: {np.mean(cross_val_score(model, X_train, y_train, cv=5)) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78793bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3303a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b76cf",
   "metadata": {},
   "source": [
    "### Enhance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9bdf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagram_grid = {'n_estimators': [5,10,15],\n",
    "               'min_samples_split':[2,3,4],\n",
    "               'n_jobs':[1,2,3]}\n",
    "\n",
    "base_estimator = RandomForestRegressor(verbose=2,max_features='log2', criterion = 'absolute_error')\n",
    "\n",
    "sh = HalvingGridSearchCV(base_estimator, pagram_grid, cv = 5,\n",
    "                         factor = 3, max_resources = 20).fit(X_train, y_train)\n",
    "sh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a43b1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sh.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda36bd3",
   "metadata": {},
   "source": [
    "Because the score after using HalvingGridSearchCV is not good as default parameters, we use default model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca549a",
   "metadata": {},
   "source": [
    "## Classification method by separating salary to some intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4adb39b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use Silhouette method to decide number of clusters\n",
    "fig, ax = plt.subplots(4, 2, figsize=(15,8))\n",
    "for i in range(2,10):\n",
    " \n",
    "    # Create KMeans instances for different number of clusters\n",
    "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "    q, mod = divmod(i, 2)\n",
    "    \n",
    "    # Create SilhouetteVisualizer instance\n",
    "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "    visualizer.fit(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a6a0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionaly, use Elbow method to decide number of clusters\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(2,10), metric='distortion', timings=False)\n",
    "\n",
    "visualizer.fit(X)        \n",
    "visualizer.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64e2e8",
   "metadata": {},
   "source": [
    "Despite showing optimized point at 4, we realize that sum of variances also can reduce when the number of cluster increases. Combining with Silhouette method, I decide to choose n_cluster = 6, where the red line goes through all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efe7c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is copied from Sklearn documentation\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(X)\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=6, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=169,\n",
    "    linewidths=3,\n",
    "    color=\"w\",\n",
    "    zorder=10,\n",
    ")\n",
    "plt.title(\n",
    "    \"K-means clustering (PCA-reduced data)\\n\"\n",
    "    \"Centroids are marked with white cross\"\n",
    ")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a46f96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.max(copp.salary_in_usd)-np.min(copp.salary_in_usd))/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93ccf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = []\n",
    "for i in range(7):\n",
    "    x = 72500*i+15000\n",
    "    interval.append(x)\n",
    "interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5626b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interval 6\n",
    "def cat_sal(x):\n",
    "    if (x >= 15000 and x < 87500):\n",
    "        return  '15000-87500'\n",
    "    elif (x >= 87500 and x < 160000):\n",
    "        return  '87500-160000'\n",
    "    elif (x >= 160000 and x < 232500):\n",
    "        return  '160000-232500'\n",
    "    elif (x >= 232500 and x < 305000):\n",
    "        return  '232500-305000'\n",
    "    elif (x >= 305000 and x < 377500):\n",
    "        return  '305000-377500'\n",
    "    else: return '377500-450000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6968a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column \"cat_sal\" for group of salary\n",
    "copp_df['cat_sal'] = copp_df[184].apply(cat_sal)\n",
    "copp_df = copp_df.drop(184, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebe06f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for ML\n",
    "X = copp_df.drop('cat_sal', axis = 1)\n",
    "y = copp_df['cat_sal']\n",
    "\n",
    "np.random.seed(100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ed6a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "         'RandomForestClassifier': RandomForestClassifier(),\n",
    "         'SVC': SVC(),\n",
    "         'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(n_components=2)}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_use = model\n",
    "    model_use.fit(X_train, y_train)\n",
    "    score = model_use.score(X_test, y_test)\n",
    "    print(f'{name} has score: {score :.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcc19e",
   "metadata": {},
   "source": [
    "Although model RandomForestClassifier has higher point than the model created before, the score is still not really good. Let's try model in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665b415",
   "metadata": {},
   "source": [
    "## Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c25e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change string type to numeric type\n",
    "le = LabelEncoder()\n",
    "y_le = le.fit_transform(y)\n",
    "\n",
    "# Convert value to torch\n",
    "X_tensor = torch.tensor(X.values).float()\n",
    "y_tensor = torch.tensor(y_le, dtype = torch.long)\n",
    "y_tensor = y_tensor[:, None]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size= 0.2)\n",
    "\n",
    "# Create train and test dataset\n",
    "train_tensor = TensorDataset(X_train, y_train)\n",
    "test_tensor = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Load data\n",
    "train_loader = DataLoader(train_tensor, batch_size=128)\n",
    "test_loader = DataLoader(test_tensor, batch_size = test_tensor.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "957893ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with a few hidden layers\n",
    "model = nn.Sequential(nn.Linear(184,150),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(150,100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100,70),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(70,40),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(40,20),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(20,6),\n",
    "                      nn.Softmax(), )\n",
    "\n",
    "# Loss function\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimize\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.2, weight_decay = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7eec82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50488419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "num_epoches = 100\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    batchAcc = []\n",
    "    batchLoss = []\n",
    "    for X_use, y_use in train_loader:\n",
    "        #forward step\n",
    "        yHat = model(X_use) \n",
    "        y_use = y_use.squeeze_() # to advoid error of multi-target\n",
    "        loss = lossfunc(yHat, y_use)\n",
    "        \n",
    "        # backward step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batchAcc.append(100 * torch.mean( (torch.argmax(yHat, axis = 1) == y_use).float() ).item() )\n",
    "        \n",
    "    trainAcc.append(np.mean(batchAcc))\n",
    "    X_test, y_test = next(iter(test_loader))\n",
    "    y_preds = model(X_test)\n",
    "    testAcc.append(100 * torch.mean( (torch.argmax(y_preds, axis = 1) == y_test).float() ).item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a719dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Train Accuracy: ', np.mean(trainAcc))\n",
    "print('Total Test Accuracy: ', np.mean(testAcc))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (10,5))\n",
    "\n",
    "ax.plot(trainAcc, label = 'train', linewidth = 2.0)\n",
    "ax.plot(testAcc, label = 'test' , linewidth = 1.0)\n",
    "ax.set_ylim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737fbc9",
   "metadata": {},
   "source": [
    "After 3 method, we conclude that using RandomClassifier gives the higher result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce1e85",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd217826",
   "metadata": {},
   "source": [
    "After conducting the EDA, some notes need to be pointed out:\n",
    "\n",
    "- The salary of most of vacancies is in the range of 110000-120000 dollar per year.\n",
    "- The most popular professions are Data Engineer, Data Scientist, Data Analyst.\n",
    "- The highest paid positions are Data Science Manager, Head of Data, Applied Scientist.\n",
    "- Due to the nature of the work, employers require candidates with advanced work experience and full-time employment.\n",
    "- Mid-sized companies hire more and pay higher wages than large and small companies.\n",
    "- The US is the country with the most hiring companies and the average salary is also the highest. Employees working in companies based in India receive lower average salaries than in the other 9 countries compared.\n",
    "- Data science positions are hiring more and more, and salaries are increasing every year.\n",
    "\n",
    "After conducting statistical hypotheses, there are differences in wages between levels of the considered variables: type of employment, experience level, company size and year. This means that all levels of each variable make an effect on salary.\n",
    "\n",
    "After using three methods, model used RandomForrestClassification algorithm gives the the highest score of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
